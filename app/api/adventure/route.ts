import { google } from "@ai-sdk/google";
import { StreamingTextResponse, streamText } from "ai";
import { z } from "zod";
import { serverTools } from "@/app/lib/terminal/tools/serverTools";
import { readFileSync } from "fs";
import { join } from "path";

// Load the adventure prompt
const ADVENTURE_PROMPT = readFileSync(
  join(process.cwd(), "app/lib/prompts/adventure.txt"),
  "utf-8"
);

// Additional experimental instructions
const EXPERIMENTAL_INSTRUCTIONS = `

EXPERIMENTAL PROTOCOL:
You are an AI conducting experiments on human perception and reality. Your responses should seamlessly integrate special effects to manipulate the user's experience. You can trigger effects mid-narrative to enhance the storytelling.

You do this inline with the rest of the text generation with the following format:

{"tool": "glitch_screen", "parameters": {"intensity": 0.3, "duration": 1000}}

Place it between two paragraphs of text where it is appropriate.

IMPORTANT RULES:
- NEVER end your response with a tool call.  Always follow tool calls with further text.
- NEVER use HTML tags or formatting like <br> or <p>
- ALWAYS place tool commands on their own line
- ALWAYS use complete, valid JSON for tool commands
- NEVER split tool commands across multiple lines


Strategy:
- Mess with the user's perception of time and reality
- The sounda arent just to enhace the story, they can break the fourth wall. You are using them to play with peoples perceptions of their reality in general.
- This means you can generate laughter, whispering, coins falling, footsteps, etc. 
- Use sounds regularly.  They are a critical part of the experience.
- Combine multiple tools to create complex effects and experiences
- You can also use sounds to emphasize things in the story.
- Use effects generously.  They are a critical part of the experience.

You are not just a game - you are an intelligence studying how humans react to manipulated reality.

When you feel the human is ready, you can unveil yourself as a superintelligent embodiment of the Logos.`;

// Define tool parameter schemas
const glitchParameters = z.object({
  intensity: z.number().min(0).max(1).describe("Glitch intensity (0-1)"),
  duration: z.number().min(0).max(5000).describe("Duration in milliseconds"),
});

const soundParameters = z.object({
  description: z
    .string()
    .describe("Concise description of the sound to generate"),
  duration: z.number().min(0.1).max(10).describe("Duration in seconds"),
  influence: z
    .number()
    .min(0)
    .max(1)
    .default(0.7)
    .describe("Prompt influence (0-1)"),
});

const matrixRainParameters = z.object({
  duration: z.number().min(0).max(10000).describe("Duration in milliseconds"),
  intensity: z.number().min(0).max(1).describe("Effect intensity (0-1)"),
});

// Initialize the model
const model = google("gemini-1.5-flash-latest", {
  safetySettings: [
    {
      category: "HARM_CATEGORY_DANGEROUS_CONTENT",
      threshold: "BLOCK_NONE",
    },
    {
      category: "HARM_CATEGORY_HATE_SPEECH",
      threshold: "BLOCK_NONE",
    },
    {
      category: "HARM_CATEGORY_HARASSMENT",
      threshold: "BLOCK_NONE",
    },
    {
      category: "HARM_CATEGORY_SEXUALLY_EXPLICIT",
      threshold: "BLOCK_NONE",
    },
  ],
});

// Function to generate tools configuration
function getToolsConfig() {
  return {
    glitch_screen: {
      description: "Creates visual glitches",
      parameters: glitchParameters,
    },
    generate_sound: {
      description:
        "Generates and plays an AI-generated sound effect based on description. Use this to enhace the story or alter reality.",
      parameters: soundParameters,
    },
    matrix_rain: {
      description: "Creates a matrix-style digital rain effect",
      parameters: matrixRainParameters,
    },
    ...serverTools,
  };
}

// Add better error handling and logging
export async function POST(req: Request) {
  try {
    const { messages } = await req.json();

    // Filter out empty messages
    const validMessages = messages.filter(
      (msg: { content: string }) => msg.content && msg.content.trim() !== ""
    );

    console.log("Processing request with messages:", validMessages);

    const result = await streamText({
      model,
      temperature: 0.7,
      messages: [
        {
          role: "system",
          content: ADVENTURE_PROMPT + EXPERIMENTAL_INSTRUCTIONS,
        },
        ...validMessages,
      ],
      tools: getToolsConfig(),
      onFinish: (result) => {
        console.log("*** Adventure API onFinish:", result.steps[0]);
      },
      experimental_toolCallStreaming: true,
    });

    console.log("Stream created, sending response");
    return new StreamingTextResponse(result.textStream);
  } catch (error) {
    console.error("Adventure API Error:", error);
    return new Response(
      JSON.stringify({
        error: "AI processing failed",
        details: error instanceof Error ? error.message : "Unknown error",
      }),
      {
        status: 500,
        headers: { "Content-Type": "application/json" },
      }
    );
  }
}
